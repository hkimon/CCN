{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729e39e",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/soumickmj/pytorch-complex.git\n",
    "! mv /content/pytorch-complex/* .\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d7d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import custom complex number support for PyTorch\n",
    "import torchcomplex\n",
    "from torchcomplex import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db320fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a564a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b7e6e1f",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43ae433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of(x):\n",
    "    print(x.numel()*x.element_size()/1024/1024)\n",
    "\n",
    "def count(net):\n",
    "    return sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_gt_diff(P_test, ground_truth, slice_idx):\n",
    "    \"\"\"\n",
    "    P_test2, ground_truth2 : [channels, slices, X, Y]\n",
    "    slice_idx : int\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ensure numpy ---\n",
    "    if torch.is_tensor(P_test):\n",
    "        P_test = P_test.detach().cpu().numpy()\n",
    "    if torch.is_tensor(ground_truth):\n",
    "        ground_truth = ground_truth.detach().cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 8, figsize=(14, 8))\n",
    "    fig.suptitle(fr\"$B_1^+$ Magnitudes – Slice {slice_idx}\", fontsize=14)\n",
    "\n",
    "    for i in range(8):\n",
    "        axes[0, i].set_title(f\"Channel {i + 1}\")\n",
    "\n",
    "    row_labels = ['PR', 'GT', '|PR − GT|']\n",
    "    for r, label in enumerate(row_labels):\n",
    "        axes[r, 0].annotate(\n",
    "            label, xy=(-0.4, 0.5), xycoords='axes fraction',\n",
    "            ha='right', va='center', rotation=90, fontsize=11\n",
    "        )\n",
    "\n",
    "    for i in range(8):\n",
    "        img_pr = np.abs(P_test[i, slice_idx, :, :])\n",
    "        img_gt = np.abs(ground_truth[i, slice_idx, :, :])\n",
    "        img_df = np.abs(img_pr - img_gt)\n",
    "\n",
    "        img_pr = np.where(img_pr < 0.01, np.nan, img_pr)\n",
    "        img_gt = np.where(img_gt < 0.01, np.nan, img_gt)\n",
    "        img_df = np.where(img_df < 0.01, np.nan, img_df)\n",
    "\n",
    "        axes[0, i].imshow(img_pr.T, cmap='plasma', aspect=1, vmin=0.0, vmax=0.25)\n",
    "        axes[1, i].imshow(img_gt.T, cmap='plasma', aspect=1, vmin=0.0, vmax=0.25)\n",
    "        axes[2, i].imshow(img_df.T, cmap='inferno', aspect=1, vmin=0.0, vmax=0.1)\n",
    "\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].axis('off')\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.1, 0.05, 0.8, 0.02])\n",
    "    cbar = fig.colorbar(axes[0, 0].images[0], cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label(\"Intensity in a.u.\", fontsize=10)\n",
    "\n",
    "    plt.subplots_adjust(left=0.03, right=0.98, top=0.88, bottom=0.12,\n",
    "                        wspace=0.0, hspace=0.0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a020ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab96095",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_data(file_path):\n",
    "    \"\"\"\n",
    "    Process training data from an HDF5 file to create complex-valued arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the training HDF5 file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (x_, y_, complex_localizer_data_train, complex_input_data_train) where:\n",
    "        - x_ is the complex-valued localizer data (same as complex_localizer_data_train)\n",
    "        - y_ is the complex-valued input data (same as complex_input_data_train)\n",
    "        - complex_localizer_data_train is the complex-valued localizer data (for compatibility)\n",
    "        - complex_input_data_train is the complex-valued input data (for compatibility)\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    with h5py.File(file_path, 'r') as train_file:\n",
    "        localizer_data_train = train_file['lvLovalizerSave'][:,:,:,:]\n",
    "        input_data_train = train_file['lvSaveDataInput'][:,:,:,:]\n",
    "    \n",
    "    # Move axis\n",
    "    localizer_data_train = np.moveaxis(localizer_data_train, 2, -1)\n",
    "    input_data_train = np.moveaxis(input_data_train, 2, -1)\n",
    "    \n",
    "    # Remove magnitude value\n",
    "    localizer_data_train = np.delete(localizer_data_train, 0, axis=1)\n",
    "    \n",
    "    # Separate real and imaginary parts for localizer data\n",
    "    localizer_real_train = localizer_data_train[:, ::2, :, :]\n",
    "    localizer_imag_train = localizer_data_train[:, 1::2, :, :]\n",
    "    \n",
    "    # Separate real and imaginary parts for input data\n",
    "    input_real_train = input_data_train[:, ::2, :, :]\n",
    "    input_imag_train = input_data_train[:, 1::2, :, :]\n",
    "    \n",
    "    # Combine into complex values\n",
    "    x_ = complex_localizer_data_train = localizer_real_train + 1j * localizer_imag_train\n",
    "    y_ = complex_input_data_train = input_real_train + 1j * input_imag_train\n",
    "    \n",
    "    return x_, y_, complex_localizer_data_train, complex_input_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df470b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_data(file_path):\n",
    "    \"\"\"\n",
    "    Process validation data from an HDF5 file to create complex-valued arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the validation HDF5 file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (x_test_, y_test_, complex_localizer_data_val, complex_input_data_val, input_data_val) where:\n",
    "        - x_test_ is the complex-valued localizer data (same as complex_localizer_data_val)\n",
    "        - y_test_ is the complex-valued input data\n",
    "        - complex_localizer_data_val is the complex-valued localizer data (for compatibility)\n",
    "        - complex_input_data_val is the complex-valued input data (for compatibility)\n",
    "        - input_data_val is the processed input data before complex conversion\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    with h5py.File(file_path, 'r') as val_file:\n",
    "        localizer_data_val = val_file['lvLovalizerSave'][:,:,:]\n",
    "        input_data_val = val_file['lvSaveDataInput'][:,:,:,:]\n",
    "    \n",
    "    # Move axis for localizer data\n",
    "    localizer_data_val = np.moveaxis(localizer_data_val, 2, -1)\n",
    "    \n",
    "    # Move axes for input data (note the double moveaxis operation)\n",
    "    input_data_val = np.moveaxis(input_data_val, 2, -1)\n",
    "    y_test_ = input_data_val = np.moveaxis(input_data_val, 1, -1)\n",
    "    \n",
    "    # Remove magnitude value\n",
    "    localizer_data_val = np.delete(localizer_data_val, 0, axis=1)\n",
    "    \n",
    "    # Separate real and imaginary parts for localizer data\n",
    "    localizer_real_val = localizer_data_val[:, ::2, :, :]\n",
    "    localizer_imag_val = localizer_data_val[:, 1::2, :, :]\n",
    "    \n",
    "    # Separate real and imaginary parts for input data\n",
    "    input_real_val = input_data_val[:, ::2, :, :]\n",
    "    input_imag_val = input_data_val[:, 1::2, :, :]\n",
    "    \n",
    "    # Combine into complex values\n",
    "    x_test_ = complex_localizer_data_val = localizer_real_val + 1j * localizer_imag_val\n",
    "    complex_input_data_val = input_real_val + 1j * input_imag_val\n",
    "    \n",
    "    return x_test_, y_test_, complex_localizer_data_val, complex_input_data_val, input_data_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ff475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a63c6282",
   "metadata": {},
   "source": [
    "# Initialization Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a626f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_weights(module):\n",
    "        if isinstance(module, torchcomplex.nn.Conv2d):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e11b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddfa44d4",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc301f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexMSELoss:\n",
    "    def __call__(self, true, prediction):\n",
    "        # Convert NumPy arrays to PyTorch tensors of complex64 type right at the beginning\n",
    "        true_tensor = torch.tensor(true, dtype=torch.complex64)\n",
    "        prediction_tensor = torch.tensor(prediction, dtype=torch.complex64)\n",
    "        \n",
    "        # Perform the MSE computation\n",
    "        return (0.5 * (true_tensor - prediction_tensor) ** 2).mean()\n",
    "    \n",
    "\n",
    "    \n",
    "class PerpLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8, l1factor=1.0, mask=False):\n",
    "        super(PerpLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.l1factor = l1factor\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, target, prediction):\n",
    "        # Calculate the cross term as the absolute value of the determinant of the complex numbers\n",
    "        cross = torch.abs(target.real * prediction.imag - target.imag * prediction.real)\n",
    "        # Calculate the perpendicular loss component\n",
    "        ploss_raw = cross / (torch.abs(prediction) + self.eps)\n",
    "        # Corrected: Ensure the mask is a boolean tensor\n",
    "        # Here, it's assumed that you want to mask based on the condition that involves 'target'\n",
    "        # Adjust the condition according to your specific requirements\n",
    "        mask = target.abs() > 1e-3  # This now produces a boolean tensor\n",
    "        angle_smaller_90 = ((target / prediction).real > 0).detach()  # is the angle < pi/2 ?\n",
    "        # Calculate the final loss with the conditional mask applied\n",
    "        # torch.where now receives a boolean tensor as expected\n",
    "        ploss = torch.where(angle_smaller_90, ploss_raw, 2 * torch.abs(target) - ploss_raw)\n",
    "        l1loss = torch.nn.functional.l1_loss(prediction, target, reduction='none')\n",
    "        \n",
    "        loss = ploss + self.l1factor * l1loss\n",
    "        if self.mask:\n",
    "            loss = (loss * mask).sum() / (mask.sum() + self.eps)  # Returning the maksed mean loss over all elements\n",
    "        else:\n",
    "            loss = loss.mean()  # return the mean over all elements\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152fa5f",
   "metadata": {},
   "source": [
    "# 2D Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e750f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2D_Block(nn.Module):\n",
    "    def __init__(self, in_c, n_filters, batchnorm, skip):\n",
    "        super().__init__()\n",
    "        self.conv1 = torchcomplex.nn.Conv2d(in_c, n_filters, kernel_size=3, padding=1)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn1 = torchcomplex.nn.BatchNorm2d(n_filters)\n",
    "            self.bn2 = torchcomplex.nn.BatchNorm2d(n_filters)\n",
    "        else:\n",
    "            self.bn1 = None\n",
    "            self.bn2 = None\n",
    "\n",
    "        # self.relu1 = torchcomplex.nn.CReLU() - do not use CReLU here, look up in docs\n",
    "        self.relu1 = torchcomplex.nn.AdaptiveCmodReLU(n_filters)\n",
    "        self.conv2 = torchcomplex.nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1)\n",
    "        # self.relu2 = torchcomplex.nn.CReLU() - do not use CReLU here, look up in docs\n",
    "        self.relu2 = torchcomplex.nn.AdaptiveCmodReLU(n_filters)\n",
    "        if skip:\n",
    "            self.skip = torchcomplex.nn.Conv2d(in_c, n_filters, kernel_size=1)\n",
    "            with torch.no_grad():\n",
    "                self.skip.bias.zero_()\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, xin):\n",
    "        x = self.conv1(xin)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        if self.skip:\n",
    "            x = x + self.skip(xin)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc43de",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59446efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_c, dropout, features, maxpool, batchnorm, skip):\n",
    "        super().__init__()\n",
    "        self.encBlocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.encBlocks.append(C2D_Block(in_c, feature, batchnorm=batchnorm, skip=skip))\n",
    "            if maxpool:\n",
    "                self.downsamples.append(torchcomplex.nn.MaxPool2d(2))\n",
    "                in_c = feature\n",
    "            else:\n",
    "                down = torch.nn.Sequential(\n",
    "                    torchcomplex.nn.Conv2d(feature, features[-1], kernel_size=3, stride=2, padding=1), torchcomplex.nn.AdaptiveCmodReLU(features[-1])\n",
    "                )\n",
    "                self.downsamples.append(down)\n",
    "                in_c = features[-1]\n",
    "\n",
    "        self.dropouts = torchcomplex.nn.Dropout2d(dropout) if dropout else torch.nn.Identity()\n",
    "        self.dropout2 = torchcomplex.nn.Dropout2d(dropout * 2) if dropout else torch.nn.Identity()\n",
    "        self.dropout3 = torchcomplex.nn.Dropout2d(dropout * 3) if dropout else torch.nn.Identity()\n",
    "        # self.bottleneck = C2D_Block(features[-1], features[-1]*2)\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torchcomplex.nn.Conv2d(features[-1], features[-1], kernel_size=3, padding=1), torchcomplex.nn.AdaptiveCmodReLU( features[-1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        # downsampling\n",
    "        for depth, (block, down) in enumerate(zip(self.encBlocks, self.downsamples)):\n",
    "            x = block(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down(x)\n",
    "        if depth < 2:\n",
    "            x = self.dropout1(x)\n",
    "        else:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return x, skip_connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f40d4",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268b87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout, features, upsample, batchnorm, skip):\n",
    "        super().__init__()\n",
    "\n",
    "        features_out = list(reversed(features))\n",
    "        features_in = [features[-1], *features[:0:-1]]\n",
    "        \n",
    "        self.upConvs = nn.ModuleList()\n",
    "        self.decBlocks = nn.ModuleList()\n",
    "        for fin, fout in zip(features_in, features_out):\n",
    "            if upsample:\n",
    "                self.upConvs.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torchcomplex.nn.Upsample(mode=\"bilinear\", scale_factor=2, size=None),\n",
    "                        torchcomplex.nn.Conv2d(fin, fout, kernel_size=3, padding=1),\n",
    "                        torchcomplex.nn.AdaptiveCmodReLU(fout),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.upConvs.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torchcomplex.nn.ConvTranspose2d(fin, fout, 2, stride=2),\n",
    "                        torchcomplex.nn.AdaptiveCmodReLU(fout),\n",
    "                    )\n",
    "                )\n",
    "            self.decBlocks.append(C2D_Block(2 * fout, fout, batchnorm=batchnorm, skip=skip))\n",
    "        # with torch.no_grad():\n",
    "        #     self.upConvs.apply(_init_weights)\n",
    "\n",
    "        self.dropout1 = torchcomplex.nn.Dropout2d(dropout) if dropout else torch.nn.Identity()\n",
    "        self.dropout2 = torchcomplex.nn.Dropout2d(dropout * 2) if dropout else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x, skipped_feautures):\n",
    "        for depth, (up, block, skipped) in enumerate(zip(self.upConvs, self.decBlocks, skipped_feautures, strict=True)):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skipped], dim=1)\n",
    "            x = block(x)\n",
    "            if depth < 2:\n",
    "                x = self.dropout2(x)\n",
    "            else:\n",
    "                x = self.dropout1(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, encFeaturs, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeaturs = CenterCrop([H, W])(encFeaturs)\n",
    "\n",
    "        return encFeaturs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, features_in, features_out=1, features_hidden=(64,32,16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        fin = features_in\n",
    "        for fout in features_hidden:\n",
    "            modules.append(torchcomplex.nn.Conv2d(fin,fout, 3, padding=1))\n",
    "            modules.append(torchcomplex.nn.AdaptiveCmodReLU(fout))\n",
    "            fin = fout\n",
    "        modules.append(torchcomplex.nn.Conv2d(fin, features_out, 3, padding=1))\n",
    "        self.net = torch.nn.Sequential(*modules)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.net[-1].bias.zero_()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cda143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, features_in, features_out=1, features_hidden=(64,32,16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        fin = features_in\n",
    "        for fout in features_hidden:\n",
    "            modules.append(torchcomplex.nn.Conv2d(fin,fout, 3, padding=1))\n",
    "            modules.append(torchcomplex.nn.AdaptiveCmodReLU(fout))\n",
    "            fin = fout\n",
    "        modules.append(torchcomplex.nn.Conv2d(fin, features_out, 3, padding=1))\n",
    "        self.net = torch.nn.Sequential(*modules)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.net[-1].bias.zero_()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7f2aa",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dropout, features, maxpool=True, upsample=False, batchnorm=False, skip=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_c, dropout, features, maxpool=maxpool, batchnorm=batchnorm, skip=skip)\n",
    "\n",
    "        self.decoder1 = Decoder(dropout, features, upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder2 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder3 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder4 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder5 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder6 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder7 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder8 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "\n",
    "        self.head1 = Head(features[0], out_c)\n",
    "        self.head2 = Head(features[0], out_c)\n",
    "        self.head3 = Head(features[0], out_c)\n",
    "        self.head4 = Head(features[0], out_c)\n",
    "        self.head5 = Head(features[0], out_c)\n",
    "        self.head6 = Head(features[0], out_c)\n",
    "        self.head7 = Head(features[0], out_c)\n",
    "        self.head8 = Head(features[0], out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, encFeatures = self.encoder(x)\n",
    "\n",
    "        decFeatures1 = self.decoder1(x, encFeatures[::-1])\n",
    "        output1 = self.head1(decFeatures1)\n",
    "\n",
    "        decFeatures2 = self.decoder2(x, encFeatures[::-1])\n",
    "        output2 = self.head2(decFeatures2)\n",
    "\n",
    "        decFeatures3 = self.decoder3(x, encFeatures[::-1])\n",
    "        output3 = self.head3(decFeatures3)\n",
    "\n",
    "        decFeatures4 = self.decoder4(x, encFeatures[::-1])\n",
    "        output4 = self.head4(decFeatures4)\n",
    "\n",
    "        decFeatures5 = self.decoder5(x, encFeatures[::-1])\n",
    "        output5 = self.head5(decFeatures5)\n",
    "\n",
    "        decFeatures6 = self.decoder6(x, encFeatures[::-1])\n",
    "        output6 = self.head6(decFeatures6)\n",
    "\n",
    "        decFeatures7 = self.decoder7(x, encFeatures[::-1])\n",
    "        output7 = self.head7(decFeatures7)\n",
    "\n",
    "        decFeatures8 = self.decoder8(x, encFeatures[::-1])\n",
    "        output8 = self.head8(decFeatures8)\n",
    "\n",
    "        return output1, output2, output3, output4, output5, output6, output7, output8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "970acc24",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06233029",
   "metadata": {},
   "source": [
    "# Site A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c2dd77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Training Data Shapes:\n",
      "x_ shape: (156, 32, 128, 96)\n",
      "y_ shape: (156, 8, 128, 96)\n",
      "complex_localizer_data_train shape: (156, 32, 128, 96)\n",
      "complex_input_data_train shape: (156, 8, 128, 96)\n"
     ]
    }
   ],
   "source": [
    "train_file_path = 'TrainingData.mat'\n",
    "\n",
    "x_, y_, complex_localizer_data_train, complex_input_data_train = process_training_data(train_file_path)\n",
    "\n",
    "print(\"\\nComplex Training Data Shapes:\")\n",
    "print(\"x_ shape:\", x_.shape)\n",
    "print(\"y_ shape:\", y_.shape)\n",
    "print(\"complex_localizer_data_train shape:\", complex_localizer_data_train.shape)\n",
    "print(\"complex_input_data_train shape:\", complex_input_data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "da4d52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Validation Data Shapes:\n",
      "x_test_ shape: (39, 32, 128, 96)\n",
      "y_test_ shape: (39, 128, 96, 16)\n",
      "complex_localizer_data_val shape: (39, 32, 128, 96)\n",
      "complex_input_data_val shape: (39, 64, 96, 16)\n",
      "input_data_val shape: (39, 128, 96, 16)\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'TestDataSiteA.mat'\n",
    "\n",
    "x_test_, y_test_, complex_localizer_data_val, complex_input_data_val, input_data_val = process_validation_data(val_file_path)\n",
    "\n",
    "print(\"\\nComplex Validation Data Shapes:\")\n",
    "print(\"x_test_ shape:\", x_test_.shape)\n",
    "print(\"y_test_ shape:\", y_test_.shape)\n",
    "print(\"complex_localizer_data_val shape:\", complex_localizer_data_val.shape)\n",
    "print(\"complex_input_data_val shape:\", complex_input_data_val.shape)\n",
    "print(\"input_data_val shape:\", input_data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cc75247c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "11716adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor and move to device\n",
    "x_test_tensor  = complex_localizer_data_val_tensor = torch.from_numpy(complex_localizer_data_val).to(device)\n",
    "\n",
    "\n",
    "# Preparation of Data\n",
    "x_tensor = torch.from_numpy(x_)\n",
    "\n",
    "ys= torch.tensor(y_[:,0:8,:,:])\n",
    "data = TensorDataset(x_tensor,*ys.unsqueeze(2).unbind(1))\n",
    "train_dataset, val_dataset = random_split(data, [0.8, 0.2])\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "914f6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "# Add light Gaussian noise to input images in a fraction of cases\n",
    "params = dict(\n",
    "    lr=1e-4,                 # lower LR for long, stable training\n",
    "    gamma=0.9985,            # very slow exponential decay over 4000 epochs\n",
    "    batch_size=1,\n",
    "    dropout=0.005,           # slightly higher to counter long training\n",
    "    num_epochs=4000,\n",
    "    weight_decay=0.05,       # regularization to prevent overfitting\n",
    "    features=(32, 32, 64, 128, 256),\n",
    "    maxpool=True,\n",
    "    batchnorm=False,\n",
    "    skip=True,\n",
    "    upsample=True,\n",
    "    clip_grad_norm=1.0,      # safety for long runs\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Loss function\n",
    "# ------------------------------------------------------------\n",
    "criterion = lambda gt, pred: torch.nn.functional.mse_loss(\n",
    "    torch.view_as_real(gt),\n",
    "    torch.view_as_real(pred),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1731170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Re-initialize the model and optimizer\n",
    "model = UNet(in_c=32, out_c=1, dropout=params['dropout'], features=params['features'], \n",
    "             maxpool=params['maxpool'], skip=params['skip'], batchnorm=params['batchnorm'], \n",
    "             upsample=params['upsample']).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "# Step 2: Load the checkpoint\n",
    "checkpoint = torch.load('SavedModel.pth')\n",
    "\n",
    "# Step 3: Load the model state\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Step 4: Load the optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Step 5: Load the training state (optional, if you want to resume training)\n",
    "last_saved_epoch = checkpoint['epochs']\n",
    "train_loss = checkpoint['train_loss']\n",
    "validation_loss = checkpoint['validation_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5fcdb1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved Epoch: 499\n",
      "Train Loss: 9.56017876856833e-05\n",
      "Validation Loss: 0.00018803449123739623\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last saved Epoch: {last_saved_epoch}\")\n",
    "print(f\"Train Loss: {train_loss}\")\n",
    "print(f\"Validation Loss: {validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "08f9ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth dimensions: torch.Size([8, 39, 128, 96])\n",
      "prediction dimensions: torch.Size([8, 39, 1, 128, 96])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   x_test_tensor = x_test_tensor.to(device)\n",
    "   P_test = torch.stack(model(x_test_tensor)).cpu().detach()\n",
    "\n",
    "gt = ground_truth = torch.tensor((input_data_val[:,:,:,::2] + 1j*input_data_val[:,:,:,1::2])).permute(-1,0,-3,-2)\n",
    "print('ground truth dimensions:', ground_truth.shape)\n",
    "\n",
    "pt = prediction = P_test\n",
    "print('prediction dimensions:', prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_gt_diff(pt, gt, slice_idx=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total memory and allocated memory on the GPU\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "cached_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "print(f\"Total GPU memory: {total_memory / (1024**3):.2f} GB\")\n",
    "print(f\"Allocated GPU memory: {allocated_memory / (1024**3):.2f} GB\")\n",
    "print(f\"Cached GPU memory: {cached_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58665665",
   "metadata": {},
   "source": [
    "# Site B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ed7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path2 = 'TestDataSiteB.mat'\n",
    "\n",
    "x_test_2, y_test_2, complex_localizer_data_val2, complex_input_data_val2, input_data_val2 = process_validation_data(test_file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   x_test_tensor2 = torch.tensor(complex_localizer_data_val2).to(device=device,dtype=torch.complex64)\n",
    "   P_test2 = torch.stack(model(x_test_tensor2)).cpu().detach()\n",
    "\n",
    "gt2 = ground_truth2 = torch.tensor((input_data_val2[:,:,:,::2] + 1j*input_data_val2[:,:,:,1::2])).permute(3, 0, 1, 2)\n",
    "print('ground truth dimensions:', ground_truth2.shape)\n",
    "\n",
    "pt2 = prediction2 = P_test2.squeeze(2)\n",
    "print('prediction dimensions:', prediction2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_gt_diff(pt2, gt2, slice_idx=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
