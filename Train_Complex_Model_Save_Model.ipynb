{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0cce56",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/soumickmj/pytorch-complex.git\n",
    "! mv /content/pytorch-complex/* .\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703fc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import custom complex number support for PyTorch\n",
    "import torchcomplex\n",
    "from torchcomplex import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83afa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35265501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb11413",
   "metadata": {},
   "source": [
    "# Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_data(train_file_path):\n",
    "    \"\"\"\n",
    "    Load training data from an HDF5 (.mat) file and convert\n",
    "    real/imaginary channel pairs into complex-valued arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_file_path : str\n",
    "        Path to the training HDF5 file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_ : np.ndarray\n",
    "        Complex-valued localizer training data.\n",
    "    y_ : np.ndarray\n",
    "        Complex-valued input (target) training data.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Load data\n",
    "    # ------------------------------------------------------------\n",
    "    with h5py.File(train_file_path, \"r\") as train_file:\n",
    "        # \"x\" data (localizer)\n",
    "        localizer_data_train = train_file[\"lvLovalizerSave\"][:, :, :, :].astype(np.float32)\n",
    "        # \"y\" data (input / target)\n",
    "        input_data_train = train_file[\"lvSaveDataInput\"][:, :, :, :].astype(np.float32)\n",
    "\n",
    "    print(\"\\nOriginal Shapes:\")\n",
    "    print(\"Training Input Data Shape:\", input_data_train.shape)\n",
    "    print(\"Training Localizer Data Shape:\", localizer_data_train.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Move channel axis: axis 2 -> last\n",
    "    # ------------------------------------------------------------\n",
    "    localizer_data_train = np.moveaxis(localizer_data_train, 2, -1)\n",
    "    input_data_train = np.moveaxis(input_data_train, 2, -1)\n",
    "\n",
    "    print(\"\\nAfter Moving Axis:\")\n",
    "    print(\"Training Input Data Shape:\", input_data_train.shape)\n",
    "    print(\"Training Localizer Data Shape:\", localizer_data_train.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Remove magnitude channel (localizer with all Tx channels on)\n",
    "    # ------------------------------------------------------------\n",
    "    localizer_data_train = np.delete(localizer_data_train, 0, axis=1)\n",
    "\n",
    "    print(\"\\nAfter Deleting Magnitude Value:\")\n",
    "    print(\"Training Localizer Data Shape:\", localizer_data_train.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Split real / imaginary parts\n",
    "    # ------------------------------------------------------------\n",
    "    # Localizer\n",
    "    localizer_real_train = localizer_data_train[:, ::2, :, :]\n",
    "    localizer_imag_train = localizer_data_train[:, 1::2, :, :]\n",
    "\n",
    "    # Input / target\n",
    "    input_real_train = input_data_train[:, ::2, :, :]\n",
    "    input_imag_train = input_data_train[:, 1::2, :, :]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Combine into complex-valued arrays\n",
    "    # ------------------------------------------------------------\n",
    "    x_ = localizer_real_train + 1j * localizer_imag_train\n",
    "    y_ = input_real_train + 1j * input_imag_train\n",
    "\n",
    "    print(\"\\nComplex Training Data Shapes:\")\n",
    "    print(\"Complex Training Localizer Data Shape:\", x_.shape)\n",
    "    print(\"Complex Training Input Data Shape:\", y_.shape)\n",
    "\n",
    "    return x_, y_\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "train_file_path = \"TrainingData.mat\"\n",
    "x_, y_ = process_training_data(train_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f718423",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_data(val_file_path):\n",
    "    \"\"\"\n",
    "    Load validation data from an HDF5 (.mat) file and convert\n",
    "    real/imaginary channel pairs into complex-valued arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    val_file_path : str\n",
    "        Path to the validation HDF5 file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_test_ : np.ndarray\n",
    "        Complex-valued localizer validation data.\n",
    "    y_test_ : np.ndarray\n",
    "        Complex-valued input (target) validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Load data\n",
    "    # ------------------------------------------------------------\n",
    "    with h5py.File(val_file_path, \"r\") as val_file:\n",
    "        # \"x_test\" data (localizer)\n",
    "        localizer_data_val = val_file[\"lvLovalizerSave\"][:, :, :]\n",
    "        # \"y_test\" data (input / target)\n",
    "        input_data_val = val_file[\"lvSaveDataInput\"][:, :, :, :]\n",
    "\n",
    "    print(\"\\nOriginal Shapes:\")\n",
    "    print(\"Validation Input Data Shape:\", input_data_val.shape)\n",
    "    print(\"Validation Localizer Data Shape:\", localizer_data_val.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Move axes\n",
    "    # ------------------------------------------------------------\n",
    "    localizer_data_val = np.moveaxis(localizer_data_val, 2, -1)\n",
    "    input_data_val = np.moveaxis(input_data_val, 2, -1)\n",
    "    input_data_val = np.moveaxis(input_data_val, 1, -1)\n",
    "\n",
    "    print(\"\\nAfter Moving Axis:\")\n",
    "    print(\"Validation Input Data Shape:\", input_data_val.shape)\n",
    "    print(\"Validation Localizer Data Shape:\", localizer_data_val.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Remove magnitude channel (localizer with all Tx channels on)\n",
    "    # ------------------------------------------------------------\n",
    "    localizer_data_val = np.delete(localizer_data_val, 0, axis=1)\n",
    "\n",
    "    print(\"\\nAfter Deleting Magnitude Value:\")\n",
    "    print(\"Validation Localizer Data Shape:\", localizer_data_val.shape)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Split real / imaginary parts\n",
    "    # ------------------------------------------------------------\n",
    "    # Localizer\n",
    "    localizer_real_val = localizer_data_val[:, ::2, :, :]\n",
    "    localizer_imag_val = localizer_data_val[:, 1::2, :, :]\n",
    "\n",
    "    # Input / target\n",
    "    input_real_val = input_data_val[:, ::2, :, :]\n",
    "    input_imag_val = input_data_val[:, 1::2, :, :]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Combine into complex-valued arrays\n",
    "    # ------------------------------------------------------------\n",
    "    x_test_ = localizer_real_val + 1j * localizer_imag_val\n",
    "    y_test_ = input_real_val + 1j * input_imag_val\n",
    "\n",
    "    print(\"\\nComplex Validation Data Shapes:\")\n",
    "    print(\"Complex Validation Localizer Data Shape:\", x_test_.shape)\n",
    "    print(\"Complex Validation Input Data Shape:\", y_test_.shape)\n",
    "\n",
    "    return x_test_, y_test_\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "val_file_path = (\"ValidationData.mat\")\n",
    "x_test_, y_test_ = process_validation_data(val_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af728b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor and move to device\n",
    "x_test_tensor = torch.from_numpy(x_test_).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fbe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "535083de",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of(x):\n",
    "    print(x.numel()*x.element_size()/1024/1024)\n",
    "\n",
    "def count(net):\n",
    "    return sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "''' Initial weights '''\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "\n",
    "def _init_weights(module):\n",
    "        if isinstance(module, torchcomplex.nn.Conv2d):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59941ad",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "\"\"\" Loss Function \"\"\"\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "class ComplexMSELoss:\n",
    "    def __call__(self, true, prediction):\n",
    "        # Convert NumPy arrays to PyTorch tensors of complex64 type right at the beginning\n",
    "        true_tensor = torch.tensor(true, dtype=torch.complex64)\n",
    "        prediction_tensor = torch.tensor(prediction, dtype=torch.complex64)\n",
    "        \n",
    "        # Perform the MSE computation\n",
    "        return (0.5 * (true_tensor - prediction_tensor) ** 2).mean()\n",
    "    \n",
    "\n",
    "    \n",
    "class PerpLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8, l1factor=1.0, mask=False):\n",
    "        super(PerpLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.l1factor = l1factor\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, target, prediction):\n",
    "        # Calculate the cross term as the absolute value of the determinant of the complex numbers\n",
    "        cross = torch.abs(target.real * prediction.imag - target.imag * prediction.real)\n",
    "        # Calculate the perpendicular loss component\n",
    "        ploss_raw = cross / (torch.abs(prediction) + self.eps)\n",
    "        # Corrected: Ensure the mask is a boolean tensor\n",
    "        # Here, it's assumed that you want to mask based on the condition that involves 'target'\n",
    "        # Adjust the condition according to your specific requirements\n",
    "        mask = target.abs() > 1e-3  # This now produces a boolean tensor\n",
    "        angle_smaller_90 = ((target / prediction).real > 0).detach()  # is the angle < pi/2 ?\n",
    "        # Calculate the final loss with the conditional mask applied\n",
    "        # torch.where now receives a boolean tensor as expected\n",
    "        ploss = torch.where(angle_smaller_90, ploss_raw, 2 * torch.abs(target) - ploss_raw)\n",
    "        l1loss = torch.nn.functional.l1_loss(prediction, target, reduction='none')\n",
    "        \n",
    "        loss = ploss + self.l1factor * l1loss\n",
    "        if self.mask:\n",
    "            loss = (loss * mask).sum() / (mask.sum() + self.eps)  # Returning the maksed mean loss over all elements\n",
    "        else:\n",
    "            loss = loss.mean()  # return the mean over all elements\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94617c96",
   "metadata": {},
   "source": [
    "# 2D Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd525ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "\"\"\" 2D Convolutional Block \"\"\"\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "\n",
    "class C2D_Block(nn.Module):\n",
    "    def __init__(self, in_c, n_filters, batchnorm, skip):\n",
    "        super().__init__()\n",
    "        self.conv1 = torchcomplex.nn.Conv2d(in_c, n_filters, kernel_size=3, padding=1)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn1 = torchcomplex.nn.BatchNorm2d(n_filters)\n",
    "            self.bn2 = torchcomplex.nn.BatchNorm2d(n_filters)\n",
    "        else:\n",
    "            self.bn1 = None\n",
    "            self.bn2 = None\n",
    "\n",
    "        # self.relu1 = torchcomplex.nn.CReLU() - do not use CReLU here, look up in docs\n",
    "        self.relu1 = torchcomplex.nn.AdaptiveCmodReLU(n_filters)\n",
    "        self.conv2 = torchcomplex.nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1)\n",
    "        # self.relu2 = torchcomplex.nn.CReLU() - do not use CReLU here, look up in docs\n",
    "        self.relu2 = torchcomplex.nn.AdaptiveCmodReLU(n_filters)\n",
    "        if skip:\n",
    "            self.skip = torchcomplex.nn.Conv2d(in_c, n_filters, kernel_size=1)\n",
    "            with torch.no_grad():\n",
    "                self.skip.bias.zero_()\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, xin):\n",
    "        x = self.conv1(xin)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        if self.skip:\n",
    "            x = x + self.skip(xin)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ca2c6",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "\"\"\" Encoder \"\"\"\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_c, dropout, features, maxpool, batchnorm, skip):\n",
    "        super().__init__()\n",
    "        self.encBlocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.encBlocks.append(C2D_Block(in_c, feature, batchnorm=batchnorm, skip=skip))\n",
    "            if maxpool:\n",
    "                self.downsamples.append(torchcomplex.nn.MaxPool2d(2))\n",
    "                in_c = feature\n",
    "            else:\n",
    "                down = torch.nn.Sequential(\n",
    "                    torchcomplex.nn.Conv2d(feature, features[-1], kernel_size=3, stride=2, padding=1), torchcomplex.nn.AdaptiveCmodReLU(features[-1])\n",
    "                )\n",
    "                self.downsamples.append(down)\n",
    "                in_c = features[-1]\n",
    "\n",
    "        self.dropouts = torchcomplex.nn.Dropout2d(dropout) if dropout else torch.nn.Identity()\n",
    "        self.dropout2 = torchcomplex.nn.Dropout2d(dropout * 2) if dropout else torch.nn.Identity()\n",
    "        self.dropout3 = torchcomplex.nn.Dropout2d(dropout * 3) if dropout else torch.nn.Identity()\n",
    "        # self.bottleneck = C2D_Block(features[-1], features[-1]*2)\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torchcomplex.nn.Conv2d(features[-1], features[-1], kernel_size=3, padding=1), torchcomplex.nn.AdaptiveCmodReLU( features[-1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        # downsampling\n",
    "        for depth, (block, down) in enumerate(zip(self.encBlocks, self.downsamples)):\n",
    "            x = block(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down(x)\n",
    "        if depth < 2:\n",
    "            x = self.dropout1(x)\n",
    "        else:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return x, skip_connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d381d7d",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48782827",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "\"\"\" Decoder \"\"\"\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout, features, upsample, batchnorm, skip):\n",
    "        super().__init__()\n",
    "\n",
    "        features_out = list(reversed(features))\n",
    "        features_in = [features[-1], *features[:0:-1]]\n",
    "        \n",
    "        self.upConvs = nn.ModuleList()\n",
    "        self.decBlocks = nn.ModuleList()\n",
    "        for fin, fout in zip(features_in, features_out):\n",
    "            if upsample:\n",
    "                self.upConvs.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torchcomplex.nn.Upsample(mode=\"bilinear\", scale_factor=2, size=None),\n",
    "                        torchcomplex.nn.Conv2d(fin, fout, kernel_size=3, padding=1),\n",
    "                        torchcomplex.nn.AdaptiveCmodReLU(fout),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.upConvs.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torchcomplex.nn.ConvTranspose2d(fin, fout, 2, stride=2),\n",
    "                        torchcomplex.nn.AdaptiveCmodReLU(fout),\n",
    "                    )\n",
    "                )\n",
    "            self.decBlocks.append(C2D_Block(2 * fout, fout, batchnorm=batchnorm, skip=skip))\n",
    "        # with torch.no_grad():\n",
    "        #     self.upConvs.apply(_init_weights)\n",
    "\n",
    "        self.dropout1 = torchcomplex.nn.Dropout2d(dropout) if dropout else torch.nn.Identity()\n",
    "        self.dropout2 = torchcomplex.nn.Dropout2d(dropout * 2) if dropout else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x, skipped_feautures):\n",
    "        for depth, (up, block, skipped) in enumerate(zip(self.upConvs, self.decBlocks, skipped_feautures, strict=True)):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skipped], dim=1)\n",
    "            x = block(x)\n",
    "            if depth < 2:\n",
    "                x = self.dropout2(x)\n",
    "            else:\n",
    "                x = self.dropout1(x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, encFeaturs, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeaturs = CenterCrop([H, W])(encFeaturs)\n",
    "\n",
    "        return encFeaturs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546aba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, features_in, features_out=1, features_hidden=(64,32,16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        fin = features_in\n",
    "        for fout in features_hidden:\n",
    "            modules.append(torchcomplex.nn.Conv2d(fin,fout, 3, padding=1))\n",
    "            modules.append(torchcomplex.nn.AdaptiveCmodReLU(fout))\n",
    "            fin = fout\n",
    "        modules.append(torchcomplex.nn.Conv2d(fin, features_out, 3, padding=1))\n",
    "        self.net = torch.nn.Sequential(*modules)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.net[-1].bias.zero_()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd9023",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---###---###---###\n",
    "\n",
    "\"\"\" UNet \"\"\"\n",
    "\n",
    "###---###---###---###\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dropout, features, maxpool=True, upsample=False, batchnorm=False, skip=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_c, dropout, features, maxpool=maxpool, batchnorm=batchnorm, skip=skip)\n",
    "\n",
    "        self.decoder1 = Decoder(dropout, features, upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder2 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder3 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder4 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder5 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder6 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder7 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "        self.decoder8 = Decoder(dropout, features ,upsample=upsample, batchnorm=batchnorm, skip=skip)\n",
    "\n",
    "        self.head1 = Head(features[0], out_c)\n",
    "        self.head2 = Head(features[0], out_c)\n",
    "        self.head3 = Head(features[0], out_c)\n",
    "        self.head4 = Head(features[0], out_c)\n",
    "        self.head5 = Head(features[0], out_c)\n",
    "        self.head6 = Head(features[0], out_c)\n",
    "        self.head7 = Head(features[0], out_c)\n",
    "        self.head8 = Head(features[0], out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, encFeatures = self.encoder(x)\n",
    "\n",
    "        decFeatures1 = self.decoder1(x, encFeatures[::-1])\n",
    "        output1 = self.head1(decFeatures1)\n",
    "\n",
    "        decFeatures2 = self.decoder2(x, encFeatures[::-1])\n",
    "        output2 = self.head2(decFeatures2)\n",
    "\n",
    "        decFeatures3 = self.decoder3(x, encFeatures[::-1])\n",
    "        output3 = self.head3(decFeatures3)\n",
    "\n",
    "        decFeatures4 = self.decoder4(x, encFeatures[::-1])\n",
    "        output4 = self.head4(decFeatures4)\n",
    "\n",
    "        decFeatures5 = self.decoder5(x, encFeatures[::-1])\n",
    "        output5 = self.head5(decFeatures5)\n",
    "\n",
    "        decFeatures6 = self.decoder6(x, encFeatures[::-1])\n",
    "        output6 = self.head6(decFeatures6)\n",
    "\n",
    "        decFeatures7 = self.decoder7(x, encFeatures[::-1])\n",
    "        output7 = self.head7(decFeatures7)\n",
    "\n",
    "        decFeatures8 = self.decoder8(x, encFeatures[::-1])\n",
    "        output8 = self.head8(decFeatures8)\n",
    "\n",
    "        return output1, output2, output3, output4, output5, output6, output7, output8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0b641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c6956e2",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad399c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Data preparation\n",
    "# ============================================================\n",
    "\n",
    "# Convert input data to PyTorch tensor\n",
    "x_tensor = torch.from_numpy(x_)\n",
    "\n",
    "# Select first 8 channels from target data\n",
    "y_tensor = torch.tensor(y_[:, 0:8, :, :])\n",
    "\n",
    "# Create TensorDataset:\n",
    "#   x_tensor        : input\n",
    "#   y_tensor split  : one tensor per Tx channel\n",
    "data = TensorDataset(\n",
    "    x_tensor,\n",
    "    *y_tensor.unsqueeze(2).unbind(1)\n",
    ")\n",
    "\n",
    "# Split into training and validation sets (80 / 20)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    data,\n",
    "    [0.8, 0.2]\n",
    ")\n",
    "\n",
    "# Concatenate back if a unified dataset is required\n",
    "dataset = ConcatDataset([train_dataset, val_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6c201",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded588f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "# Add light Gaussian noise to input images in a fraction of cases\n",
    "params = dict(\n",
    "    lr=1e-4,                 # lower LR for long, stable training\n",
    "    gamma=0.9985,            # very slow exponential decay over 4000 epochs\n",
    "    batch_size=1,\n",
    "    dropout=0.005,           # slightly higher to counter long training\n",
    "    num_epochs=4000,\n",
    "    weight_decay=0.05,       # regularization to prevent overfitting\n",
    "    features=(32, 32, 64, 128, 256),\n",
    "    maxpool=True,\n",
    "    batchnorm=False,\n",
    "    skip=True,\n",
    "    upsample=True,\n",
    "    clip_grad_norm=1.0,      # safety for long runs\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Loss function\n",
    "# ------------------------------------------------------------\n",
    "criterion = lambda gt, pred: torch.nn.functional.mse_loss(\n",
    "    torch.view_as_real(gt),\n",
    "    torch.view_as_real(pred),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f6a60",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Reproducibility\n",
    "# ============================================================\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# Model\n",
    "# ============================================================\n",
    "model = UNet(\n",
    "    in_c=32,\n",
    "    out_c=1,\n",
    "    dropout=params[\"dropout\"],\n",
    "    features=params[\"features\"],\n",
    "    maxpool=params[\"maxpool\"],\n",
    "    skip=params[\"skip\"],\n",
    "    batchnorm=params[\"batchnorm\"],\n",
    "    upsample=params[\"upsample\"],\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{n_params / 1e6:.2f} Mio.\")\n",
    "print(\"number_trainable_parameters =\", n_params)\n",
    "print(model)\n",
    "\n",
    "# If criterion is a callable/loss instance, this is just informational\n",
    "try:\n",
    "    print(\"criterion =\", criterion.__class__.__name__)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ============================================================\n",
    "# Train/val split (via samplers)\n",
    "# ============================================================\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "start_time = time.time()\n",
    "\n",
    "train_idx, val_idx = torch.utils.data.random_split(\n",
    "    torch.arange(len(dataset)),\n",
    "    (0.9, 0.1),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    sampler=SubsetRandomSampler(train_idx),\n",
    "    num_workers=10,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    sampler=SubsetRandomSampler(val_idx),\n",
    "    num_workers=10,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Optimizer + scheduler\n",
    "# ============================================================\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=params[\"lr\"],\n",
    "    weight_decay=params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=params[\"gamma\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Training loop\n",
    "# ============================================================\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ------------------------\n",
    "    # Train\n",
    "    # ------------------------\n",
    "    model.train()\n",
    "    train_epoch_loss = 0.0\n",
    "\n",
    "    for step, (x, *y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = [yi.to(device) for yi in y]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(x)  # expected: iterable/list of heads\n",
    "        head_losses = [criterion(out_i, y_i) for out_i, y_i in zip(outputs, y)]\n",
    "        loss = sum(head_losses) / len(head_losses)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), params[\"clip_grad_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_epoch_loss / len(train_loader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "\n",
    "    # LR step (printed explicitly)\n",
    "    lr_before = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    lr_after = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Epoch {epoch + 1:4d}/{params['num_epochs']} | train_loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"lr: {lr_before:.6e} -> {lr_after:.6e}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Validation\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, *y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = [yi.to(device) for yi in y]\n",
    "\n",
    "            outputs = model(x)\n",
    "            head_losses = [criterion(out_i, y_i) for out_i, y_i in zip(outputs, y)]\n",
    "            val_loss = sum(head_losses) / len(head_losses)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = val_epoch_loss / len(val_loader)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"val_loss: {avg_val_loss:.6f} | epoch_time: {epoch_time:.2f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# Done\n",
    "# ============================================================\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n[INFO] total time taken to train the model: {total_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6ffbc",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epochs\": epoch,\n",
    "        \"parameters\": params,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_loss\": train_loss[-1],\n",
    "        \"validation_loss\": valid_loss[-1],\n",
    "    },\n",
    "    'SaveModel.pth',\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
