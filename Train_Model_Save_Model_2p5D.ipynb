{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729219fc",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a727bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Imports & GPU configuration\n",
    "# =============================\n",
    "import os\n",
    "\n",
    "# Set before importing tensorflow\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # Use CPU only; set to \"0\" or \"1\" to use GPU(s)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, BatchNormalization,\n",
    "    ReLU, LeakyReLU, concatenate\n",
    ")\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback, ModelCheckpoint, LearningRateScheduler, CSVLogger\n",
    ")\n",
    "\n",
    "import einops\n",
    "\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Visible devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d025b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96cc614",
   "metadata": {},
   "source": [
    "# Slice Stacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data_semi3d(data_set):\n",
    "    data_set = einops.rearrange(data_set,\"(subject slices) ...->subject slices ...\", slices=13)\n",
    "    data_set = np.lib.stride_tricks.sliding_window_view(data_set, 3, axis=1)\n",
    "    data_set = einops.rearrange(data_set,\"subject sample x y channel z -> (subject sample) x y z channel\")\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef53ab",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda76c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Training data\n",
    "# =============================\n",
    "\n",
    "with h5py.File(\"TrainData.mat\", \"r\") as f:\n",
    "    y_train = f[\"lvSaveDataInput\"][:, :, :, :]\n",
    "    x_train = f[\"lvLovalizerSave\"][:, :, :, :]\n",
    "\n",
    "# Move channel axis (1 → last)\n",
    "x_train = np.moveaxis(x_train, 1, -1)\n",
    "y_train = np.moveaxis(y_train, 1, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# important for model building later on\n",
    "input_shape  = x_train.shape[1:]   \n",
    "output_shape = y_train.shape[1:]      \n",
    "\n",
    "print(\"Input shape:\", input_shape)\n",
    "print(\"Output shape:\", output_shape)\n",
    "\n",
    "# =============================\n",
    "\n",
    "x_train = stack_data_semi3d(x_train)\n",
    "y_train = stack_data_semi3d(y_train)[...,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Validation data\n",
    "# =============================\n",
    "\n",
    "with h5py.File(\"ValData.mat\", \"r\") as f:\n",
    "    y_val = f[\"lvSaveDataInput\"][:, :, :, :]\n",
    "    x_val = f[\"lvLovalizerSave\"][:, :, :, :]\n",
    "\n",
    "# Move channel axis (1 → last)\n",
    "x_val = np.moveaxis(x_val, 1, -1)\n",
    "y_val = np.moveaxis(y_val, 1, -1)\n",
    "\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "\n",
    "# =============================\n",
    "\n",
    "x_val = stack_data_semi3d(x_val)\n",
    "y_val = stack_data_semi3d(y_val)[..., 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Testing data\n",
    "# =============================\n",
    "\n",
    "with h5py.File(\"TestData.mat\", \"r\") as f:\n",
    "    y_test = f[\"lvSaveDataInput\"][:, :, :, :]\n",
    "    x_test = f[\"lvLovalizerSave\"][:, :, :, :]\n",
    "\n",
    "# Move channel axis (1 → last)\n",
    "x_test = np.moveaxis(x_test, 1, -1)\n",
    "y_test = np.moveaxis(y_test, 1, -1)\n",
    "\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# =============================\n",
    "\n",
    "x_test = stack_data_semi3d(x_test)\n",
    "y_test = stack_data_semi3d(y_test)[..., 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c16658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6b406c",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae690d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perpendicular_loss(target, prediction, eps=1e-8, l1factor=1.0, use_mask=False):\n",
    "    \"\"\"\n",
    "    Perpendicular loss for complex-valued tensors:\n",
    "    total = P + l1factor * L1\n",
    "\n",
    "    target, prediction: complex tensors of same shape\n",
    "    returns: scalar loss (mean over all elements, masked if use_mask=True)\n",
    "    \"\"\"\n",
    "\n",
    "    # cross term = |target.real * pred.imag - target.imag * pred.real|\n",
    "    t_real = tf.math.real(target)\n",
    "    t_imag = tf.math.imag(target)\n",
    "    p_real = tf.math.real(prediction)\n",
    "    p_imag = tf.math.imag(prediction)\n",
    "\n",
    "    cross = tf.abs(t_real * p_imag - t_imag * p_real)  # real, >=0\n",
    "\n",
    "    # |prediction|\n",
    "    abs_pred = tf.abs(prediction)\n",
    "    abs_target = tf.abs(target)\n",
    "\n",
    "    # perpendicular component (P_raw)\n",
    "    ploss_raw = cross / (abs_pred + eps)\n",
    "\n",
    "    # angle < 90° ?  (Re(target / prediction) > 0)\n",
    "    ratio = target / prediction\n",
    "    angle_smaller_90 = tf.math.real(ratio) > 0  # boolean\n",
    "\n",
    "    # symmetric perpendicular loss P\n",
    "    ploss = tf.where(angle_smaller_90,\n",
    "                     ploss_raw,\n",
    "                     2.0 * abs_target - ploss_raw)\n",
    "\n",
    "    # L1 component\n",
    "    l1loss = tf.abs(prediction - target)\n",
    "\n",
    "    # P + L1\n",
    "    total = ploss + l1factor * l1loss\n",
    "\n",
    "    if use_mask:\n",
    "        mask = abs_target > 1e-3  # boolean\n",
    "        mask = tf.cast(mask, total.dtype)\n",
    "        masked_sum = tf.reduce_sum(total * mask)\n",
    "        mask_count = tf.reduce_sum(mask)\n",
    "        loss = masked_sum / (mask_count + eps)\n",
    "    else:\n",
    "        loss = tf.reduce_mean(total)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b4604",
   "metadata": {},
   "source": [
    "# Custom Learning-Rate Metric, Exponential Decay Scheduler, and Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "       return optimizer.learning_rate# optimizer.lr\n",
    "    return lr\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.99944 \n",
    "    decay_step = 1\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Ensure logs is not None and contains the required keys\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        if \"loss\" in logs and \"val_loss\" in logs:\n",
    "            print(\n",
    "                \"loss: {:.4e} - \"\n",
    "                \"val_loss: {:.4e}\".format(\n",
    "                    logs[\"loss\"], logs[\"val_loss\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch {epoch} ended but logs are missing expected keys: {logs}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67791a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a2a03bd",
   "metadata": {},
   "source": [
    "# Double Convolutional Block 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(n_filters, kernel_size=3, batchnorm=True):\n",
    "    \"\"\"\n",
    "    Creates a reusable convolutional block as a Sequential model.\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        Conv2D(\n",
    "            filters=n_filters, \n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            kernel_initializer=init,\n",
    "            padding=\"same\"\n",
    "        )\n",
    "    ]\n",
    "    if batchnorm:\n",
    "        layers.append(BatchNormalization())\n",
    "    layers.append(ReLU())\n",
    "    #layers.append(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    layers.append(\n",
    "        Conv2D(\n",
    "            filters=n_filters, \n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            kernel_initializer=init,\n",
    "            padding=\"same\"\n",
    "        )\n",
    "    )\n",
    "    if batchnorm:\n",
    "        layers.append(BatchNormalization())\n",
    "    layers.append(ReLU())\n",
    "    #layers.append(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    \n",
    "    return Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def C2D_BLock(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    \"\"\"\n",
    "    Create a convolutional block with two Conv2D layers.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor from previous layer\n",
    "        n_filters (int): Number of filters for convolution\n",
    "        kernel_size (int): Size of convolution kernel (default: 3)\n",
    "        batchnorm (bool): Whether to apply batch normalization (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor after two convolution operations\n",
    "    \"\"\"\n",
    "    # First convolution layer\n",
    "    x = Conv2D(\n",
    "        filters=n_filters, \n",
    "        kernel_size=(kernel_size, kernel_size), \n",
    "        kernel_initializer=init,\n",
    "        padding=\"same\"\n",
    "    )(input_tensor)\n",
    "    \n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "   #x = LeakyReLU(alpha=0.2)(x)\n",
    "  \n",
    "    # Second convolution layer\n",
    "    x = Conv2D(\n",
    "        filters=n_filters, \n",
    "        kernel_size=(kernel_size, kernel_size), \n",
    "        kernel_initializer=init,\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    \n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    x = ReLU()(x)\n",
    "   #x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036a424",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f61167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_semi3D(x, convs, dropout):\n",
    "    \"\"\"Simple encoder with configurable iterations.\"\"\"\n",
    "    features = []\n",
    "    for i,conv in enumerate(convs):\n",
    "        # Convolution block\n",
    "        down = conv(x)\n",
    "        features.append(down)\n",
    "        \n",
    "        # Max pooling (except for last iteration - that's the bottleneck)\n",
    "        if i < len(convs)-1:\n",
    "            x = MaxPooling2D((2, 2))(down)\n",
    "            # Dropout with increasing rate for deeper layers\n",
    "            dropout_rate = dropout * (2 if i >= 2 else 1)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "        else:\n",
    "            # Bottleneck dropout\n",
    "            x = Dropout(dropout * 3)(down)\n",
    "            features[-1] = x  # Update last feature with dropout\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_slice_encoder_semi3D(input_img, convolution_type, iterations, n_filters, dropout, batchnorm):\n",
    "    \"\"\"Process multi-slice input through encoder.\"\"\"\n",
    "    slices = tf.unstack(input_img, axis=-2)\n",
    "    all_features = []\n",
    "    print(\"filters\", [n_filters*2**i for i in range(iterations)])\n",
    "    convs = [convolution_type(n_filters*2**i, 3, batchnorm) for i in range(iterations)]\n",
    "    for s in slices:\n",
    "        # If slice is (batch, H, W) → make it (batch, H, W, 1)\n",
    "        #if s.shape.rank == 3:\n",
    "        #    s = tf.expand_dims(s, axis=-1)\n",
    "        all_features.append(\n",
    "            encoder_semi3D(s, convs, dropout)\n",
    "        )\n",
    "\n",
    "    combined_features = [\n",
    "        Conv2D(n_filters * 2**depth, 1)(\n",
    "            tf.concat([current_slice[depth] for current_slice in all_features], axis=-1)\n",
    "        )\n",
    "        for depth in range(iterations)\n",
    "    ]\n",
    "    print(combined_features)\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728a7a3",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16004e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(bottleneck, skip_connections, convolution_type, transpose_conv_type, iterations, n_filters, dropout, batchnorm, heads=8):\n",
    "    \"\"\"Simple decoder with multiple heads.\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    for head in range(heads):\n",
    "        x = bottleneck\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Calculate filter size (decreasing: 8, 4, 2, 1)\n",
    "            current_filters = n_filters * (2 ** (iterations - i - 1))\n",
    "            \n",
    "            # Upsampling with configurable transpose convolution\n",
    "            x = transpose_conv_type(current_filters, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "            x = concatenate([x, skip_connections[i]])\n",
    "            \n",
    "            # Dropout (higher rate for first 2 iterations)\n",
    "            dropout_rate = dropout * (2 if i < 2 else 1)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "            \n",
    "            # Convolution block\n",
    "            x = convolution_type(x, n_filters=current_filters, kernel_size=3, batchnorm=batchnorm)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Conv2D(2, (1, 1), activation='tanh', name=f\"outputsCh{head+1}\")(x)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d7ad6",
   "metadata": {},
   "source": [
    "# U Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_unet_semi3D(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
    "    \n",
    "    features = multi_slice_encoder_semi3D(\n",
    "        input_img=input_img,\n",
    "        convolution_type=ConvBlock,\n",
    "        iterations=5,\n",
    "        n_filters=n_filters,\n",
    "        dropout=dropout,\n",
    "        batchnorm=batchnorm,\n",
    "    )\n",
    "    down1, down2, down3, down4, down5 = features\n",
    "    \n",
    "    outputs = decoder(  # ← Use your new function with the fix\n",
    "        bottleneck=down5,\n",
    "        skip_connections=[down4, down3, down2, down1],\n",
    "        convolution_type=C2D_BLock,\n",
    "        transpose_conv_type=Conv2DTranspose,\n",
    "        iterations=4,\n",
    "        n_filters=n_filters,\n",
    "        dropout=dropout,\n",
    "        batchnorm=batchnorm,\n",
    "        heads=8,\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs=[input_img], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19582480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60eddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8693857",
   "metadata": {},
   "source": [
    "# Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aaba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Weight initialization\n",
    "# ============================================================\n",
    "# Small random initialization helps stabilize early training\n",
    "init = RandomNormal(stddev=0.02)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Checkpointing & callbacks\n",
    "# ============================================================\n",
    "# Path where the best model weights will be stored\n",
    "checkpoint_path = \"checks.weights.h5\"\n",
    "\n",
    "# Save best weights (based on training loss) every 100 batches\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    "    save_freq=100,   # NOTE: save frequency is in *batches*, not epochs\n",
    ")\n",
    "\n",
    "# Callback list:\n",
    "# - ModelCheckpoint: saves best weights\n",
    "# - LearningRateScheduler: applies custom LR decay\n",
    "# - CustomCallback: prints loss / val_loss at epoch end\n",
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "    LearningRateScheduler(lr_scheduler),\n",
    "    CustomCallback(),\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model definition\n",
    "# ============================================================\n",
    "# Input tensor\n",
    "input_img = Input(input_shape)\n",
    "\n",
    "# 2D U-Net architecture\n",
    "model = define_unet_semi3D(\n",
    "    input_img,\n",
    "    n_filters=32,\n",
    "    dropout=0.001,\n",
    "    batchnorm=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Optimizer & metrics\n",
    "# ============================================================\n",
    "optimizer = Adam(\n",
    "    learning_rate=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-7,\n",
    "    clipnorm=1.0,    # Gradient clipping for training stability\n",
    "    amsgrad=False,\n",
    ")\n",
    "\n",
    "# Expose current learning rate as a metric (for logging)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compile model\n",
    "# ============================================================\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\n",
    "        \"mse\",       # Standard MSE metric\n",
    "        lr_metric,   # Learning rate tracking\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30a82e",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08772f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model training\n",
    "# ============================================================\n",
    "history = model.fit(\n",
    "    x=x_train,   # input localizer images\n",
    "    y={\n",
    "        # Each output head predicts one Tx channel (complex: real + imag)\n",
    "        \"outputsCh1\": y_train[:, :, :,  0: 2],\n",
    "        \"outputsCh2\": y_train[:, :, :,  2: 4],\n",
    "        \"outputsCh3\": y_train[:, :, :,  4: 6],\n",
    "        \"outputsCh4\": y_train[:, :, :,  6: 8],\n",
    "        \"outputsCh5\": y_train[:, :, :,  8:10],\n",
    "        \"outputsCh6\": y_train[:, :, :, 10:12],\n",
    "        \"outputsCh7\": y_train[:, :, :, 12:14],\n",
    "        \"outputsCh8\": y_train[:, :, :, 14:16],\n",
    "    },\n",
    "    validation_data=(\n",
    "        x_val,     # validation inputs\n",
    "        {\n",
    "            \"outputsCh1\": y_val[:, :, :,  0: 2],\n",
    "            \"outputsCh2\": y_val[:, :, :,  2: 4],\n",
    "            \"outputsCh3\": y_val[:, :, :,  4: 6],\n",
    "            \"outputsCh4\": y_val[:, :, :,  6: 8],\n",
    "            \"outputsCh5\": y_val[:, :, :,  8:10],\n",
    "            \"outputsCh6\": y_val[:, :, :, 10:12],\n",
    "            \"outputsCh7\": y_val[:, :, :, 12:14],\n",
    "            \"outputsCh8\": y_val[:, :, :, 14:16],\n",
    "        },\n",
    "    ),\n",
    "    shuffle=True,          # shuffle samples each epoch\n",
    "    epochs=4000,           # long training for convergence\n",
    "    batch_size=1,          # slice-wise training\n",
    "    callbacks=callbacks_list,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98013c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ebedab",
   "metadata": {},
   "source": [
    "# Save model weights & training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model weights\n",
    "model.save_weights('Model_Weights.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history (losses, metrics, LR, etc.)\n",
    "np.save('History', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d41a28c1",
   "metadata": {},
   "source": [
    "# Inference on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(model.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d425877",
   "metadata": {},
   "source": [
    "# Save complex-valued ground truth (unseen / validation data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Ground-truth B1+ maps (complex-valued)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Reconstruct complex-valued ground truth from real/imag pairs\n",
    "b1p_groundtruth_complex = y_val[..., 0::2] + 1j * y_val[..., 1::2]\n",
    "\n",
    "# Save complex-valued ground truth for downstream analysis\n",
    "np.save(\"b1p_gt_complex.npy\", b1p_groundtruth_complex)\n",
    "\n",
    "# Derived representations (channel-first for convenience)\n",
    "b1p_gt_magnitude = np.moveaxis(np.abs(b1p_groundtruth_complex),   -1, 0)\n",
    "b1p_gt_phase     = np.moveaxis(np.angle(b1p_groundtruth_complex), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786aac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Predicted B1+ maps (complex-valued)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Reconstruct complex-valued prediction from real/imag pairs\n",
    "\n",
    "b1p_prediction_complex = prediction[..., 0] + 1j * prediction[..., 1]\n",
    "\n",
    "# Save complex-valued prediction for downstream analysis\n",
    "np.save(\"b1p_pr_complex.npy\", b1p_prediction_complex)\n",
    "\n",
    "# Derived representations (channel-first for convenience)\n",
    "b1p_pr_magnitude = np.moveaxis(np.abs(b1p_prediction_complex),   -1, 0)\n",
    "b1p_pr_phase     = np.moveaxis(np.angle(b1p_prediction_complex), -1, 0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
